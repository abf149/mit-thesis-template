%% This is an example first chapter.  You should put chapter/appendix that you
%% write into a separate file, and add a line \include{yourfilename} to
%% main.tex, where `yourfilename.tex' is the name of the chapter/appendix file.
%% You can process specific files by typing their names in at the 
%% \files=
%% prompt when you run the file main.tex through LaTeX.
\chapter{Introduction}
\label{chapter:introduction}

Accelerating tensor operations is a significant research area with applications in deep neural network (DNN)-based machine learning inference\cite{eyeriss}\cite{eyerissv2}\cite{tpu}\cite{extensor}, scientific computing\cite{sci_tensor}, graph analytics\cite{mattson2013standards}, and data science\cite{mcauley2013hidden}\cite{kolda2009tensor}\cite{bader2008discussion}. Industrial demand for large scale\cite{tpu} and/or energy-efficient\cite{eyeriss} DNN deployment, combined with the demise of Moore's Law\cite{moore}, motivated the first generation of DNN-oriented tensor accelerators\cite{eyeriss}\cite{tpu}.

More recently, a new generation of hardware accelerators exploit tensor sparsity for improved scalability and efficiency\cite{ampere}\cite{eyerissv2}\cite{sparten}\cite{sparch}\cite{scnn}\cite{candles}\cite{extensor}. Tensor zero-compression \cite{szebook} \cite{sparseloop} \cite{extensor} and data-dependent computation strategies such as gating and skipping \cite{szebook} \cite{sparseloop} can meaningfully impact memory footprint, energy, area and total cycle time of an accelerator computation\cite{szebook}\cite{sparseloop}.

By removing elements, tensor zero-compression removes the regular structure that makes trivial operations such as iteration and co-iteration through tensor ranks feasible. While this problem is resolved with a layer of indirection (i.e. by adding \textit{metadata} that can recover the original coordinates of non-zero elements\cite{szebook}), the resulting sparse-tensor versions of these operations are sometimes more complex and resource intensive than their dense tensor counterparts\cite{extensor}\cite{sparten}\cite{sparch}\cite{ampere}. 

An additional avenue for exploiting sparsity is to optimize away arithmetic operations \cite{eyerissv2} \cite{sparten}\cite{extensor} \cite{sparch} \cite{szebook} \cite{sparseloop} (or quiesce compute during ineffectual operations (IneffOps)\cite{eyeriss}\cite{sparseloop}\cite{szebook}), the cost being a more complex microarchitecture that implements the sparsity-optimized arithmetic\cite{eyeriss}\cite{eyerissv2}. Generally speaking, to exploit sparsity for gains on key metrics, and still create a functionally correct accelerator, researchers frequently are compelled to specialize traversal\cite{szebook}\cite{extensor}, contraction\cite{gamma}\cite{eyerissv2}\cite{extensor}\cite{sparten}, or transposition/shuffle\cite{gamma} microarchitectures to be compatible with the architecture and its sparse representation format(s). Sometimes sparsity creates the need for microarchitectural functions that an equivalent dense tensor accelerator might not require for a similar workload, such as managing memory conflicts\cite{scnn}\cite{sparten}. New specialized microarchitectures for exploiting sparsity have played a key role in the advancement of sparse tensor accelerator research\cite{gamma}\cite{outerspace}\cite{extensor}\cite{sparch}\cite{outerspace}\cite{ampere}.

Despite the volume of sparse tensor accelerator research, existing work lacks systematic design-space exploration and apples-to-apples comparison between alternative microarchitecture proposals. This likely reflects that researchers are frequently innovating many areas simultaneously - architecture, dataflow, sparse representation format, and microarchitecture to name a few\cite{gamma}\cite{matraptor}\cite{eyeriss}\cite{candles}. With so much research and so much variety, it is no wonder sparse tensor microarchitecture seems to have evaded meaningful categorization; prior work lacks a consistent set of abstractions that would facilitate design-space exploration or apple-to-apples comparison.

In contrast, accelerator \textit{architecture} has proven amenable to categorization systems that enable modeling, design-space exploration and apples-to-apples comparison between designs\cite{timeloop}\cite{sparseloop}\cite{accelergy}\cite{buffet}. For example, Timeloop\cite{timeloop} is a design-space exploration framework for dense DNN accelerators. Timeloop relies on a consistent set of abstractions for common architectural blocks, i.e. MAC, network-on-chip (NoC), Buffer (which can be subclassed as SRAM, DRAM, register file, ...), etc. Timeloop is used alongside Accelergy\cite{accelergy}, a pre-RTL analytical modeling framework, in order to explore the space of dense accelerator designs and rank them by cost against key metrics.

Sparseloop\cite{sparseloop} is a sparsity extension for Timeloop. Sparseloop accepts an abstract, declarative specification of one or more desired sparsity optimizations (known as a Sparse Acceleration Feature or SAF.) SAFs are bound to architectural buffers or arithmetic units by the user, and may be used to signal format optimization (tensor zero-compression) or action optimizations (gating or skipping of memory accesses, or gating or skipping of compute operations)\cite{sparseloop}. To estimate energy savings for an action optimization, the idealized SAF is lowered to align with the architectural component model it applies to, resulting in a decrease in the number of actions against that architectural component. In theory, Sparseloop could also lower the SAF to align with a model of SAF microarchitecture energy-per-action, yielding an estimate of how the microarchitectural cost of the optimization trades off against its benefit in reducing architectural energy consumption. In practice, Sparseloop lacks models of microarchitectural primitives and design topologies. When it comes to design-space exploration, SAF microarchitectures are neither factored into the computation energy estimate, nor into the accelerator area estimate.

This work attempts to synthesize a number of prior works into a concise, unified, and effective framework for doing research on SAF microarchitectures. The overall framework developed here comprises (1) a conceptual framework which facilitates concise description, modeling and design-space exploration for SAF microarchitectures, (2) a software framework, SAFTools, for compiling Sparseloop-style SAF descriptions into microarchitecture designs and analytical models, and (3) an extensible component library including specific SAF microarchitecture subcomponent designs as well as RTL to support implementation. SAFTools yields pre-RTL analytical models which are compatible with Accelergy and Sparseloop, and which hook into Accelergy architectural buffer models in such a way, that architectural buffer actions are translated into actions against a model of SAF microarchitecture energy. Furthermore, SAF microarchitecture area is factored into total design area. 

As a first step toward a set of consistent abstractions and modeling tools for SAF microarchitecture primitives - analogous to what is currently available for architectural modeling - it is hoped that this work will help enable researchers to systematically explore and compare sparsity optimizations in the design of sparse tensor accelerators. Section~\ref{chapter:background} motivates SAF microarchitecture research and provides context. Section~\ref{chapter:conceptual_framework} builds a novel conceptual framework for describing SAF microarchitectures. Section~\ref{chapter:rtl} overviews the RTL component designs (``RTL blocks'') which were written and characterized in order to support model development (additionally, these RTL blocks comprise the RTL library associated with this work.) Section~\ref{chapter:modeling} introduces low-level modeling abstractions that account for the relationship between workload and component parameterization. Section~\ref{chapter:primitive_taxo_model} introduces the taxonomy of SAF microarchitecture primitives and the associated analytical models. Building on the previous section, Section~\ref{chapter:saf_microarchitectures} introduces the taxonomy of SAF microarchitecture compound components and shows how SAFs are concretized into SAF microarchitectures. Section~\ref{chapter:framework} overviews the architecture of the SAFtools software, component libraries, and RTL block libraries. Section~\ref{chapter:evaluation} evaluates SAFTools. Section~\ref{chapter:case_studies} provides a case-study of a hypothetical sparse tensor accelerator with SAF microarchitecture.

%\begin{table}[ht]
%\resizebox{\textwidth}{!}{%
%\begin{tabular}{c|p{2.5cm}|p{2.5cm}|p{2.5cm}|p{2.5cm}}
% & Modeling speed & Accurate SAF $\mu$architecture modeling & Consistent SAF $\mu$architecture abstractions? & Open-source SAF $\mu$architecture RTL?\\ \hline \hline
%Architectural modeling frameworks & \textcolor{green}{\textbf{Fast}} & \textcolor{red}{\textbf{No}} & \textcolor{red}{\textbf{No}} & \textcolor{red}{\textbf{No}} \\ \hline
%Design-specific models & \textcolor{red}{\textbf{Slow}} & \textcolor{green}{\textbf{Yes}} & \textcolor{red}{\textbf{No}} & \textcolor{red}{\textbf{Limited}} \\ \hline
%$\mu$architecture taxonomy papers & \textcolor{red}{\textbf{N/A}} & \textcolor{red}{\textbf{No}} & \textcolor{red}{\textbf{Limited}} & \textcolor{red}{\textbf{No}} \\ \hline
%\textbf{This work} & \textcolor{green}{\textbf{Fast}} & \textcolor{green}{\textbf{Yes}} & \textcolor{green}{\textbf{Yes}} & \textcolor{green}{\textbf{Yes}} \\ \hline
%\end{tabular}
%}
%\label{tab:thiswork}
%\caption{SAFTools and the underlying SAF microarchitecture taxonomy enable fast, accurate SAF microarchitecture modeling based on a consistent set of abstractions.}
%\centering
%\end{table}