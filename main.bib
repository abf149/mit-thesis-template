@misc{hanintro,
  title = {TinyML and
Efficient Deep Learning Computing},
  howpublished = {\url{https://hanlab.mit.edu/files/course/slides/MIT-TinyML-Lec01-Introduction.pdf}},
  note = {Accessed: 2023-05-01}
}

@misc{frameworks,
  title = {Accelerating Inference Up to 6x Faster in PyTorch with Torch-TensorRT},
  howpublished = {\url{https://developer.nvidia.com/blog/accelerating-inference-up-to-6x-faster-in-pytorch-with-torch-tensorrt/}},
  note = {Accessed: 2023-05-01}
}

@misc{ampereblog,
  title = {Accelerating Inference with Sparsity Using the NVIDIA Ampere Architecture and NVIDIA TensorRT},
  howpublished = {\url{https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/}},
  note = {Accessed: 2023-05-01}
}

@misc{archuarch,
  title = {Architecture and micro-architecture},
  howpublished = {\url{https://developer.arm.com/documentation/102404/0200/Architecture-and-micro-architecture}},
  note = {Accessed: 2023-05-01}
}

@misc{archuarch,
  title = {Architecture and micro-architecture},
  howpublished = {\url{https://developer.arm.com/documentation/102404/0200/Architecture-and-micro-architecture}},
  note = {Accessed: 2023-05-01}
}

@misc{examplearch,
      title={ThUnderVolt: Enabling Aggressive Voltage Underscaling and Timing Error Resilience for Energy Efficient Deep Neural Network Accelerators}, 
      author={Jeff Zhang and Kartheek Rangineni and Zahra Ghodsi and Siddharth Garg},
      year={2018},
      eprint={1802.03806},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}

@INPROCEEDINGS{accelergy,  author={Wu, Yannan Nellie and Emer, Joel S. and Sze, Vivienne},  booktitle={2019 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)},   title={Accelergy: An Architecture-Level Energy Estimation Methodology for Accelerator Designs},   year={2019},  volume={},  number={},  pages={1-8},  doi={10.1109/ICCAD45719.2019.8942149}}

@inproceedings{extensor,
author = {Hegde, Kartik and Asghari-Moghaddam, Hadi and Pellauer, Michael and Crago, Neal and Jaleel, Aamer and Solomonik, Edgar and Emer, Joel and Fletcher, Christopher W.},
title = {ExTensor: An Accelerator for Sparse Tensor Algebra},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358275},
doi = {10.1145/3352460.3358275},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {319–333},
numpages = {15},
keywords = {Tensor Algebra, Sparse Computation, Hardware Acceleration},
location = {Columbus, OH, USA},
series = {MICRO '52}
}

@article{scnn,
author = {Parashar, Angshuman and Rhu, Minsoo and Mukkara, Anurag and Puglielli, Antonio and Venkatesan, Rangharajan and Khailany, Brucek and Emer, Joel and Keckler, Stephen and Dally, William},
year = {2017},
month = {05},
pages = {},
title = {SCNN: An Accelerator for Compressed-sparse Convolutional Neural Networks},
volume = {45},
journal = {ACM SIGARCH Computer Architecture News},
doi = {10.1145/3140659.3080254}
}

@INPROCEEDINGS{timeloop,  author={Parashar, Angshuman and Raina, Priyanka and Shao, Yakun Sophia and Chen, Yu-Hsin and Ying, Victor A. and Mukkara, Anurag and Venkatesan, Rangharajan and Khailany, Brucek and Keckler, Stephen W. and Emer, Joel},  booktitle={2019 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)},   title={Timeloop: A Systematic Approach to DNN Accelerator Evaluation},   year={2019},  volume={},  number={},  pages={304-315},  doi={10.1109/ISPASS.2019.00042}}

@INPROCEEDINGS{sparseloop,  author={Wu, Yannan Nellie and Tsai, Po-An and Parashar, Angshuman and Sze, Vivienne and Emer, Joel S.},  booktitle={2021 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)},   title={Sparseloop: An Analytical, Energy-Focused Design Space Exploration Methodology for Sparse Tensor Accelerators},   year={2021},  volume={},  number={},  pages={232-234},  doi={10.1109/ISPASS51385.2021.00043}}

@inproceedings{sparten,
author = {Gondimalla, Ashish and Chesnut, Noah and Thottethodi, Mithuna and Vijaykumar, T. N.},
title = {SparTen: A Sparse Tensor Accelerator for Convolutional Neural Networks},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358291},
doi = {10.1145/3352460.3358291},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {151–165},
numpages = {15},
keywords = {Sparse tensors, Convolutional neural networks, Accelerators},
location = {Columbus, OH, USA},
series = {MICRO '52}
}

@inproceedings{gamma,
author = {Zhang, Guowei and Attaluri, Nithya and Emer, Joel S. and Sanchez, Daniel},
title = {Gamma: Leveraging Gustavson’s Algorithm to Accelerate Sparse Matrix Multiplication},
year = {2021},
isbn = {9781450383172},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3445814.3446702},
doi = {10.1145/3445814.3446702},
booktitle = {Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {687–701},
numpages = {15},
keywords = {explicit data orchestration, accelerator, high-radix merge, data movement reduction, sparse linear algebra, sparse matrix multiplication, Gustavson's algorithm},
location = {Virtual, USA},
series = {ASPLOS 2021}
}

@INPROCEEDINGS{matraptor,  author={Srivastava, Nitish and Jin, Hanchen and Liu, Jie and Albonesi, David and Zhang, Zhiru},  booktitle={2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)},   title={MatRaptor: A Sparse-Sparse Matrix Multiplication Accelerator Based on Row-Wise Product},   year={2020},  volume={},  number={},  pages={766-780},  doi={10.1109/MICRO50266.2020.00068}}

@ARTICLE{eyerissv2,  author={Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel and Sze, Vivienne},  journal={IEEE Journal on Emerging and Selected Topics in Circuits and Systems},   title={Eyeriss v2: A Flexible Accelerator for Emerging Deep Neural Networks on Mobile Devices},   year={2019},  volume={9},  number={2},  pages={292-308},  doi={10.1109/JETCAS.2019.2910232}}

@INPROCEEDINGS{sigma,  author={Qin, Eric and Samajdar, Ananda and Kwon, Hyoukjun and Nadella, Vineet and Srinivasan, Sudarshan and Das, Dipankar and Kaul, Bharat and Krishna, Tushar},  booktitle={2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)},   title={SIGMA: A Sparse and Irregular GEMM Accelerator with Flexible Interconnects for DNN Training},   year={2020},  volume={},  number={},  pages={58-70},  doi={10.1109/HPCA47549.2020.00015}}

@INPROCEEDINGS{dsstc,  author={Wang, Yang and Zhang, Chen and Xie, Zhiqiang and Guo, Cong and Liu, Yunxin and Leng, Jingwen},  booktitle={2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)},   title={Dual-side Sparse Tensor Core},   year={2021},  volume={},  number={},  pages={1083-1095},  doi={10.1109/ISCA52012.2021.00088}}

@inproceedings{candles,
  title={CANDLES: Channel-Aware Novel Dataflow-Microarchitecture Co-Design for Low Energy Sparse Neural Network Acceleration},
  author={Sumanth Gudaparthi and Sarabjeet Singh},
  year={2021}
}

@INPROCEEDINGS{sparch,  author={Zhang, Zhekai and Wang, Hanrui and Han, Song and Dally, William J.},  booktitle={2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)},   title={SpArch: Efficient Architecture for Sparse Matrix Multiplication},   year={2020},  volume={},  number={},  pages={261-274},  doi={10.1109/HPCA47549.2020.00030}}

@INPROCEEDINGS{eie,  author={Han, Song and Liu, Xingyu and Mao, Huizi and Pu, Jing and Pedram, Ardavan and Horowitz, Mark A. and Dally, William J.},  booktitle={2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA)},   title={EIE: Efficient Inference Engine on Compressed Deep Neural Network},   year={2016},  volume={},  number={},  pages={243-254},  doi={10.1109/ISCA.2016.30}}

@INPROCEEDINGS{outerspace,  author={Pal, Subhankar and Beaumont, Jonathan and Park, Dong-Hyeon and Amarnath, Aporva and Feng, Siying and Chakrabarti, Chaitali and Kim, Hun-Seok and Blaauw, David and Mudge, Trevor and Dreslinski, Ronald},  booktitle={2018 IEEE International Symposium on High Performance Computer Architecture (HPCA)},   title={OuterSPACE: An Outer Product Based Sparse Matrix Multiplication Accelerator},   year={2018},  volume={},  number={},  pages={724-736},  doi={10.1109/HPCA.2018.00067}}

@article{szebook,
  title={Efficient processing of deep neural networks},
  author={Sze, Vivienne and Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel S},
  journal={Synthesis Lectures on Computer Architecture},
  volume={15},
  number={2},
  pages={1--341},
  year={2020},
  publisher={Morgan \& Claypool Publishers}
}

@inproceedings{eyeriss,
    author      = {{Chen, Yu-Hsin and Krishna, Tushar and Emer, Joel and Sze, Vivienne}},
    title       = {{Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks}},
    booktitle   = {{IEEE International Solid-State Circuits Conference, ISSCC 2016, Digest of Technical Papers}},
    year        = {2016},
    pages       = {{262-263}},
}

@techreport{ampere,
     title = {{NVIDIA AMPERE GA102 GPU 
ARCHITECTURE}},
     institution = {NVIDIA},
}

@inproceedings{pruning,
  title={Optimal Brain Damage},
  author={Yann LeCun and John S. Denker and Sara A. Solla},
  booktitle={NIPS},
  year={1989}
}

@inproceedings{mergertaxo,
author = {Wang, Bangyan and Deng, Lei and Sun, Fei and Dai, Guohao and Liu, Liu and Wang, Yu and Xie, Yuan},
title = {A One-for-All and <i>o</i>(<i>v</i> Log(<i>v</i> ))-Cost Solution for Parallel Merge Style Operations on Sorted Key-Value Arrays},
year = {2022},
isbn = {9781450392051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503222.3507728},
doi = {10.1145/3503222.3507728},
booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {669–682},
numpages = {14},
keywords = {Join, Graph, SpGEMM, Sparse linear algebra, SIMD, Key-value array, Merge sort},
location = {Lausanne, Switzerland},
series = {ASPLOS 2022}
}

@ARTICLE{koggestone,
  author={Kogge, Peter M. and Stone, Harold S.},
  journal={IEEE Transactions on Computers}, 
  title={A Parallel Algorithm for the Efficient Solution of a General Class of Recurrence Equations}, 
  year={1973},
  volume={C-22},
  number={8},
  pages={786-793},
  doi={10.1109/TC.1973.5009159}}

@misc{recursive_priority_encoder, title={Recursive modules}, url={https://www.beyond-circuits.com/wordpress/2009/01/recursive-modules/}, journal={Beyond Circuits}, author={Johnson, Pete and Podolsky, Igor}, year={2014}, month={Oct}} 

@INPROCEEDINGS{chisel,
  author={Bachrach, Jonathan and Vo, Huy and Richards, Brian and Lee, Yunsup and Waterman, Andrew and Avižienis, Rimas and Wawrzynek, John and Asanović, Krste},
  booktitle={DAC Design Automation Conference 2012}, 
  title={Chisel: Constructing hardware in a Scala embedded language}, 
  year={2012},
  volume={},
  number={},
  pages={1212-1221},
  doi={10.1145/2228360.2228584}}

@techreport{firrtl,
  title = {Specification for the FIRRTL Language},
  author = {Patrick S. Li and Adam M. Izraelevitz and Jonathan Bachrach},
  year = {2016},
  month = {Feb},
  institution = {EECS Department, University of California, Berkeley},
  number = {UCB/EECS-2016-9},
  note = {Cited on page 43}
}

@article{verilog_history,
author = {Flake, Peter and Moorby, Phil and Golson, Steve and Salz, Arturo and Davidmann, Simon},
year = {2020},
month = {06},
pages = {1-90},
title = {Verilog HDL and its ancestors and descendants},
volume = {4},
journal = {Proceedings of the ACM on Programming Languages},
doi = {10.1145/3386337}
}

@inproceedings{systemc,
author = {Panda, Preeti},
year = {2001},
month = {02},
pages = {75- 80},
title = {SystemC - a modeling platform supporting multiple design abstractions},
isbn = {1-58113-418-5},
doi = {10.1109/ISSS.2001.156535}
}

@INPROCEEDINGS{bluespec,
  author={Nikhil, R.},
  booktitle={Proceedings. Second ACM and IEEE International Conference on Formal Methods and Models for Co-Design, 2004. MEMOCODE '04.}, 
  title={Bluespec System Verilog: efficient, correct RTL from high level specifications}, 
  year={2004},
  volume={},
  number={},
  pages={69-70},
  doi={10.1109/MEMCOD.2004.1459818}}

@INPROCEEDINGS{wattch,
  author={Brooks, D. and Tiwari, V. and Martonosi, M.},
  booktitle={Proceedings of 27th International Symposium on Computer Architecture (IEEE Cat. No.RS00201)}, 
  title={Wattch: a framework for architectural-level power analysis and optimizations}, 
  year={2000},
  volume={},
  number={},
  pages={83-94},
  doi={}}

@inproceedings{tensorflow,
  title={TensorFlow: A system for large-scale machine learning},
  author={Abadi, Mart{\'i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  booktitle={OSDI'16},
  year={2016}
}
 
@inproceedings{tpu,title	= {In-Datacenter Performance Analysis of a Tensor Processing Unit},author	= {Norman P. Jouppi and Cliff Young and Nishant Patil and David Patterson and Gaurav Agrawal and Raminder Bajwa and Sarah Bates and Suresh Bhatia and Nan Boden and Al Borchers and Rick Boyle and Pierre-luc Cantin and Clifford Chao and Chris Clark and Jeremy Coriell and Mike Daley and Matt Dau and Jeffrey Dean and Ben Gelb and Tara Vazir Ghaemmaghami and Rajendra Gottipati and William Gulland and Robert Hagmann and C. Richard Ho and Doug Hogberg and John Hu and Robert Hundt and Dan Hurt and Julian Ibarz and Aaron Jaffey and Alek Jaworski and Alexander Kaplan and Harshit Khaitan and Andy Koch and Naveen Kumar and Steve Lacy and James Laudon and James Law and Diemthu Le and Chris Leary and Zhuyuan Liu and Kyle Lucke and Alan Lundin and Gordon MacKean and Adriana Maggiore and Maire Mahony and Kieran Miller and Rahul Nagarajan and Ravi Narayanaswami and Ray Ni and Kathy Nix and Thomas Norrie and Mark Omernick and Narayana Penukonda and Andy Phelps and Jonathan Ross},year	= {2017},URL	= {https://arxiv.org/pdf/1704.04760.pdf}}

@inproceedings{mattson2013standards,
  title={Standards for graph algorithm primitives},
  author={Mattson, Timothy and Bader, David and Berry, Jonathan and Bulu{\c{c}}, Ayd{\i}n and Dongarra, Jack and Faloutsos, Christos and Feo, John and Gilbert, John and Gonzalez, Joseph and Hendrickson, Bruce and Kepner, Jeremy and Leiserson, Charles and Lumsdaine, Andrew and Padua, David and Poole, Stephen and Reinhardt, Steven and Stonebraker, Michael and Wallach, Steve and Yoo, Andrew},
  booktitle={HPEC'13},
  year={2013}
}

@inproceedings{bader2008discussion,
  title={Discussion tracking in Enron email using PARAFAC},
  author={Bader, Brett W and Berry, Michael W and Browne, Murray},
  booktitle={Survey of Text Mining II},
  pages={147--163},
  year={2008},
  publisher={Springer}
}

@article{kolda2009tensor,
  title={Tensor decompositions and applications},
  author={Kolda, Tamara G and Bader, Brett W},
  journal={SIAM Review},
  year={2009},
  publisher={SIAM}
}

@inproceedings{mcauley2013hidden,
  title={Hidden factors and hidden topics: understanding rating dimensions with review text},
  author={McAuley, Julian and Leskovec, Jure},
  booktitle={Proceedings of the 7th ACM conference on Recommender systems},
  organization={ACM},
  year={2013}
}

@article{sci_tensor,
author = {Di Napoli, Edoardo and Bientinesi, Paolo and Li, Jiajia and Uschmajew, André},
year = {2022},
month = {09},
pages = {1038885},
title = {Editorial: High-performance tensor computations in scientific computing and data science},
volume = {8},
journal = {Frontiers in Applied Mathematics and Statistics},
doi = {10.3389/fams.2022.1038885}
}

@article{survey_dnn_acceleration,
title = {A Survey of Accelerator Architectures for Deep Neural Networks},
journal = {Engineering},
volume = {6},
number = {3},
pages = {264-274},
year = {2020},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2020.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S2095809919306356},
author = {Yiran Chen and Yuan Xie and Linghao Song and Fan Chen and Tianqi Tang},
keywords = {Deep neural network, Domain-specific architecture, Accelerator},
abstract = {Recently, due to the availability of big data and the rapid growth of computing power, artificial intelligence (AI) has regained tremendous attention and investment. Machine learning (ML) approaches have been successfully applied to solve many problems in academia and in industry. Although the explosion of big data applications is driving the development of ML, it also imposes severe challenges of data processing speed and scalability on conventional computer systems. Computing platforms that are dedicatedly designed for AI applications have been considered, ranging from a complement to von Neumann platforms to a “must-have” and stand-alone technical solution. These platforms, which belong to a larger category named “domain-specific computing,” focus on specific customization for AI. In this article, we focus on summarizing the recent advances in accelerator designs for deep neural networks (DNNs)—that is, DNN accelerators. We discuss various architectures that support DNN executions in terms of computing units, dataflow optimization, targeted network topologies, architectures on emerging technologies, and accelerators for emerging applications. We also provide our visions on the future trend of AI chip designs.}
}

@Inbook{moore,
author="Mahmoud, Dina",
editor="Mulder, Valentin
and Mermoud, Alain
and Lenders, Vincent
and Tellenbach, Bernhard",
title="Hardware Acceleration",
bookTitle="Trends in Data Protection and Encryption Technologies ",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="109--114",
abstract="With Moore's law and Dennard's scaling no longer fueling the improvement in computing performance, new avenues for increasing performance are needed. Hardware acceleration is one avenue where many researchers and industrial parties are working and investing. Accelerators can allow for high levels of parallelism not supported by general-purpose central processing units. These high levels of parallelism are particularly well-suited for many modern applications. Therefore, research on the use of hardware acceleration is expected to continue in the near future. However, various parties should consider a variety of aspects when deciding whether to invest in hardware acceleration by making their own accelerators or by buying them from a third party. This factsheet presents an analysis of hardware acceleration and the trends until 2025. It also discusses the aspects and how specific considerations are more important for some actors.",
isbn="978-3-031-33386-6",
doi="10.1007/978-3-031-33386-6_20",
url="https://doi.org/10.1007/978-3-031-33386-6_20"
}

@inproceedings{buffet,
author = {Pellauer, Michael and Shao, Yakun Sophia and Clemons, Jason and Crago, Neal and Hegde, Kartik and Venkatesan, Rangharajan and Keckler, Stephen W. and Fletcher, Christopher W. and Emer, Joel},
title = {Buffets: An Efficient and Composable Storage Idiom for Explicit Decoupled Data Orchestration},
year = {2019},
isbn = {9781450362405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297858.3304025},
doi = {10.1145/3297858.3304025},
abstract = {Accelerators spend significant area and effort on custom on-chip buffering. Unfortunately, these solutions are strongly tied to particular designs, hampering re-usability across other accelerators or domains. We present buffets, an efficient and composable storage idiom for the needs of accelerators that is independent of any particular design. Buffets have several distinguishing characteristics, including efficient decoupled fills and accesses with fine-grained synchronization, hierarchical composition, and efficient multi-casting. We implement buffets in RTL and show that they only add 2\% control overhead over an 8KB RAM. When compared with DMA-managed double-buffered scratchpads and caches across a range of workloads, buffets improve energy-delay-product by 1.53x and 5.39x, respectively.},
booktitle = {Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {137–151},
numpages = {15},
keywords = {staging buffers, data orchestration, accelerators, synchronization},
location = {Providence, RI, USA},
series = {ASPLOS '19}
}

@INPROCEEDINGS{cambricon_x,
  author={Zhang, Shijin and Du, Zidong and Zhang, Lei and Lan, Huiying and Liu, Shaoli and Li, Ling and Guo, Qi and Chen, Tianshi and Chen, Yunji},
  booktitle={2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)}, 
  title={Cambricon-X: An accelerator for sparse neural networks}, 
  year={2016},
  volume={},
  number={},
  pages={1-12},
  doi={10.1109/MICRO.2016.7783723}}

@INPROCEEDINGS{cnvlutin,
  author={Albericio, Jorge and Judd, Patrick and Hetherington, Tayler and Aamodt, Tor and Jerger, Natalie Enright and Moshovos, Andreas},
  booktitle={2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA)}, 
  title={Cnvlutin: Ineffectual-Neuron-Free Deep Neural Network Computing}, 
  year={2016},
  volume={},
  number={},
  pages={1-13},
  doi={10.1109/ISCA.2016.11}}

@misc{sparse_arch_lecture1,
  title={Hardware Architectures for Deep Learning: Sparse Architectures – Part 1},
  author={Emer, Joel and Sze, Vivienne},
  year={2023},
  month={April},
  day={5},
  howpublished={Massachusetts Institute of Technology, Electrical Engineering \& Computer Science},
  url={http://csg.csail.mit.edu/6.5930/Lectures/L15.pdf}
}

@article{taco_format,
  author       = {Stephen Chou and
                  Fredrik Kjolstad and
                  Saman P. Amarasinghe},
  title        = {Unified Sparse Formats for Tensor Algebra Compilers},
  journal      = {CoRR},
  volume       = {abs/1804.10112},
  year         = {2018},
  url          = {http://arxiv.org/abs/1804.10112},
  eprinttype    = {arXiv},
  eprint       = {1804.10112},
  timestamp    = {Mon, 13 Aug 2018 16:47:32 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1804-10112.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{teaal,
      title={TeAAL: A Declarative Framework for Modeling Sparse Tensor Accelerators}, 
      author={Nandeeka Nayak and Toluwanimi O. Odemuyiwa and Shubham Ugare and Christopher W. Fletcher and Michael Pellauer and Joel S. Emer},
      year={2023},
      eprint={2304.07931},
      archivePrefix={arXiv},
      primaryClass={cs.AR}
}

@inproceedings{csf,
author = {Smith, Shaden and Karypis, George},
title = {Tensor-Matrix Products with a Compressed Sparse Tensor},
year = {2015},
isbn = {9781450340014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2833179.2833183},
doi = {10.1145/2833179.2833183},
abstract = {The Canonical Polyadic Decomposition (CPD) of tensors is a powerful tool for analyzing multi-way data and is used extensively to analyze very large and extremely sparse datasets. The bottleneck of computing the CPD is multiplying a sparse tensor by several dense matrices. Algorithms for tensor-matrix products fall into two classes. The first class saves floating point operations by storing a compressed tensor for each dimension of the data. These methods are fast but suffer high memory costs. The second class uses a single uncompressed tensor at the cost of additional floating point operations. In this work, we bridge the gap between the two approaches and introduce the compressed sparse fiber (CSF) a data structure for sparse tensors along with a novel parallel algorithm for tensor-matrix multiplication. CSF offers similar operation reductions as existing compressed methods while using only a single tensor structure. We validate our contributions with experiments comparing against state-of-the-art methods on a diverse set of datasets. Our work uses 58\% less memory than the state-of-the-art while achieving 81\% of the parallel performance on 16 threads.},
booktitle = {Proceedings of the 5th Workshop on Irregular Applications: Architectures and Algorithms},
articleno = {5},
numpages = {7},
location = {Austin, Texas},
series = {IA3 '15}
}

@article{taco,
  author = {Kjolstad, Fredrik and Kamil, Shoaib and Chou, Stephen and Lugato, David and Amarasinghe, Saman},
  title = {The Tensor Algebra Compiler},
  journal = {Proc. ACM Program. Lang.},
  issue_date = {October 2017},
  volume = {1},
  number = {OOPSLA},
  month = oct,
  year = {2017},
  issn = {2475-1421},
  pages = {77:1--77:29},
  articleno = {77},
  numpages = {29},
  url = {http://doi.acm.org/10.1145/3133901},
  doi = {10.1145/3133901},
  acmid = {3133901},
  publisher = {ACM},
  address = {New York, NY, USA},
  keywords = {code generation, iteration graphs, linear algebra, merge lattices, parallelism, performance, sparse data structures, tensor algebra, tensors}
}

@INPROCEEDINGS {tensaurus,
author = {N. Srivastava and H. Jin and S. Smith and H. Rong and D. Albonesi and Z. Zhang},
booktitle = {2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
title = {Tensaurus: A Versatile Accelerator for Mixed Sparse-Dense Tensor Computations},
year = {2020},
volume = {},
issn = {},
pages = {689-702},
abstract = {Tensor factorizations are powerful tools in many machine learning and data analytics applications. Tensors are often sparse, which makes sparse tensor factorizations memory bound. In this work, we propose a hardware accelerator that can accelerate both dense and sparse tensor factorizations. We co-design the hardware and a sparse storage format, which allows accessing the sparse data in vectorized and streaming fashion and maximizes the utilization of the memory bandwidth. We extract a common computation pattern that is found in numerous matrix and tensor operations and implement it in the hardware. By designing the hardware based on this common compute pattern, we can not only accelerate tensor factorizations but also mixed sparse-dense matrix operations. We show significant speedup and energy benefit over the state-of-the-art CPU and GPU implementations of tensor factorizations and over CPU, GPU and accelerators for matrix operations.},
keywords = {tensile stress;sparse matrices;hardware;kernel;acceleration;matrix decomposition;bandwidth},
doi = {10.1109/HPCA47549.2020.00062},
url = {https://doi.ieeecomputersociety.org/10.1109/HPCA47549.2020.00062},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {feb}
}

@misc{smartbuffer,
    title = {{smart-buffer}},
    howpublished = {\url{https://www.npmjs.com/package/smart-buffer}},
    note = {Accessed: January 25 2024},
    year = {2022}
}


@misc{freepdk,
    title = {{FreePDK45 TM}},
    howpublished = {\url{https://eda.ncsu.edu/freepdk/freepdk45/}},
    note = {Accessed: January 25, 2024},
    year = {2011}
}
